{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download objects and pathes form various sources\n",
    "\n",
    "The data collected uses data from Austria. The used sources can be used for other countries as well. \n",
    "\n",
    "**Generally the data is not guaranteed to be complete or accurate.**\n",
    "\n",
    "\n",
    "## Wikipedia\n",
    "\n",
    "Data is downloaded to find addresses and match coordinates. This helps in geolocating various points of interest.\n",
    "\n",
    "Trainstations parsed from wikipedia are not used, the data quatility of OSM is better\n",
    "\n",
    "## OpenStreetMap\n",
    "\n",
    "Data is downloaded via overpass queries The results are points of interests (and pathes)\n",
    "\n",
    "See https://overpass-turbo.eu/# for checking and constructing queries\n",
    "\n",
    "<!-- ## LLM Services -->\n",
    "\n",
    "<!-- Are used to check some of the data, and impute missing values (e.g. coordinates) -->\n",
    "\n",
    "## Postprocessing Steps\n",
    "\n",
    "The raw data is generally cached when possible, to avoid unnecessary requests and improve performance.\n",
    "\n",
    "\n",
    "- **Filtering**: Removing irrelevant or duplicate data entries.\n",
    "\n",
    "- **Normalizing**: Standardizing data formats for consistency.\n",
    "\n",
    "- **Storing**: Saving the processed data in JSON format for easy access and manipulation.\n",
    "\n",
    "\n",
    "## Additional Resources:\n",
    "\n",
    "* GIS Austria -> https://gis.stmk.gv.at/wgportal/atlasmobile/map/Basiskarten/Basiskarte\n",
    "\n",
    "    Also flooding zones are available here\n",
    "\n",
    "* https://www.geonames.org/\n",
    "\n",
    "* statistik.at\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopy.distance as gd\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import datautils\n",
    "\n",
    "import simplekml\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "from fastkml import kml\n",
    "\n",
    "\n",
    "# create folder structures and holds constants\n",
    "data_path = datautils.DataPaths(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl = \"https://de.wikipedia.org/wiki/Liste_der_Eisenbahnh%C3%B6fe_und_-haltestellen_in_%C3%96sterreich\"\n",
    "table_class = \"wikitable sortable jquery-tablesorter\"\n",
    "response = requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "wikitables = soup.find_all(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "print(f\" # of tables {len(wikitables)}\")\n",
    "# print(wikitables[1])\n",
    "\n",
    "# trainstation_table\n",
    "pd_trainstations_partitial = []\n",
    "for index, wikitable in enumerate(wikitables):\n",
    "    if index == 0:\n",
    "        print(\"skip first\")\n",
    "        continue\n",
    "    df = pd.read_html(str(wikitable), header=0)[0]\n",
    "    pd_trainstations_partitial.append(df)\n",
    "\n",
    "print(f\"parsed tables {len(pd_trainstations_partitial)}\")\n",
    "pd_trainstations = pd.concat(pd_trainstations_partitial, ignore_index=True)\n",
    "\n",
    "print(f\"columns {pd_trainstations.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_trainstations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd_trainstations)\n",
    "current_year = datetime.now().year\n",
    "pd_trainstations.to_csv(f\"{data_path.wikipedia}/austria_trainstations_{datetime.now().year}.csv\", sep=';')\n",
    "pd_trainstations_styria = pd_trainstations.query(\"BL == 'St'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://nominatim.openstreetmap.org/search?q=Wien,+Austria&format=json&polygon=1&addressdetails=1\n",
    "server = 'https://nominatim.openstreetmap.org'\n",
    "#s = requests.session()\n",
    "#s.config['keep_alive'] = False\n",
    "#headers = {'Accept': 'application/json', 'Content-Type': 'application/json', \"user-agent\": \"python-requests/2.9.1\", \"HTTP Referer\": \"http://nominatim.openstreetmap.org\"}\n",
    "headers = {\"user-agent\": \"python-requests/2.9.1\"}\n",
    "\n",
    "def find_coords(name:str):\n",
    "    print(name)\n",
    "    esc_name = name.replace(' ', '+')\n",
    "    print(esc_name)\n",
    "    url = f\"{server}/search?q={esc_name},Österreich&format=json&polygon=1&addressdetails=1\"\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    #response = requests.get(\"%s/search?q=%s,Österreich&format=json&polygon=1&addressdetails=1\" % (server, esc_name), headers=headers)\n",
    "\n",
    "    if not response.status_code == 200:\n",
    "        print(\"error: service not reachable; error:\", response.status_code)\n",
    "        exit(1)\n",
    "\n",
    "    #headers_json = {'Accept': 'application/json', 'Content-Type': 'application/json', 'token': token}\n",
    "    #with open(artifacts_file, 'rb') as f:\n",
    "    #    data = f.read()\n",
    "\n",
    "    #response = requests.post(\"%s/secure/asset/clientupdate\" % server, headers=headers_json, data=data)\n",
    "\n",
    "    #json_request_content = json.loads(data)\n",
    "    #json_request_content.pop(\"installedModules\", None)\n",
    "\n",
    "    print(response.text)\n",
    "\n",
    "    json_content = json.loads(response.text)\n",
    "    if len(json_content) == 0:\n",
    "        return (False, 0, 0)\n",
    "    \n",
    "    lat = (float)(json_content[0]['lat'])\n",
    "    lon = (float)(json_content[0]['lon'])\n",
    "    bb = (json_content[0]['boundingbox'])\n",
    "    height = float(bb[1])-float(bb[0])\n",
    "    width = float(bb[3])-float(bb[2])\n",
    "\n",
    "    print(len(json_content))\n",
    "    \n",
    "    return (True, lat, lon, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skip_trainstations = True\n",
    "not_found_trainstations = []\n",
    "found_coords = {}\n",
    "found_coords_list = []\n",
    "\n",
    "# reset X and Y\n",
    "pd_trainstations['X'] = float('Nan')\n",
    "pd_trainstations['Y'] = float('Nan')\n",
    "\n",
    "if skip_trainstations == False:\n",
    "    # iterate all trainsations in Styria and find the coordinates\n",
    "    for index, trainstation in pd_trainstations[pd_trainstations[\"BL\"] == \"St\"].iterrows():\n",
    "        station_name = trainstation[\"Name\"]\n",
    "\n",
    "        while True:\n",
    "            search_station = (station_name + \",\") if len(station_name) > 0 else \"\"\n",
    "            print(index, search_station)\n",
    "            search = f\"Bahnhof,{search_station}{trainstation['Standort­gemeinde']}\"\n",
    "\n",
    "            coords = find_coords(f\"{search}\")\n",
    "\n",
    "            if coords[0]:\n",
    "                found_coords[trainstation[\"Name\"]] = [coords[1], coords[2]]\n",
    "                found_coords_list.append([coords[1], coords[2]])\n",
    "                print(coords)\n",
    "                pd_trainstations.at[index, \"X\"] = coords[1]\n",
    "                pd_trainstations.at[index, \"Y\"] = coords[2]\n",
    "                break\n",
    "            else:\n",
    "                had_split_char = False\n",
    "                if \", \" in station_name:\n",
    "                    station_name = station_name.split(\", \", 1)[1]\n",
    "                    had_split_char = True\n",
    "                elif \" \" in station_name:\n",
    "                    station_name = station_name.split(\" \", 1)[1]\n",
    "                    had_split_char = True\n",
    "\n",
    "            if len(station_name) == 0:\n",
    "                print(\"not found\")\n",
    "                not_found_trainstations.append((trainstation[\"Name\"], trainstation[\"Art\"]))\n",
    "                found_coords_list.append([float(\"Nan\"), float(\"Nan\")])\n",
    "                break\n",
    "\n",
    "            if had_split_char == False:\n",
    "                station_name = \"\"\n",
    "                continue\n",
    "\n",
    "        time.sleep(0.8) # needed by the provider of the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not_found_trainstations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_trainstations.query(\"BL == 'St'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_trainstations.sort_values(by='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_poi(query: str, tagname: str, use_cache: bool = True):\n",
    "    server = \"https://lz4.overpass-api.de/api/interpreter\"\n",
    "    query = \"\\n\".join(\n",
    "        [line for line in query.split(\"\\n\") if not line.strip().startswith(\"//\")]\n",
    "    )\n",
    "    esc_query = query.replace(\"\\n\", \"\").replace(\" \", \"+\")\n",
    "\n",
    "    # not perfekt to set the object here, but better than link it on every call\n",
    "    cache_dir = f\"{data_path.cache}/boundary/place\"\n",
    "    datautils.create_folders([cache_dir])\n",
    "\n",
    "    if os.path.exists(cache_dir) == False:\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    cache_file = os.path.join(cache_dir, f\"{tagname}.json\")\n",
    "\n",
    "    if use_cache and os.path.exists(cache_file):\n",
    "        # if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\", encoding=\"utf8\") as f:\n",
    "            data = f.read()\n",
    "            print(f\"returning data '{cache_file}' from cache ({len(data)} bytes)\")\n",
    "            return data\n",
    "\n",
    "    url = f\"{server}?data={esc_query}\"\n",
    "    print(url)\n",
    "    headers = {\"user-agent\": \"python-requests/2.9.1\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if not response.status_code == 200:\n",
    "        print(\n",
    "            f\"error: service not reachable; error: {response.status_code}, {response}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # store\n",
    "    content = response.text\n",
    "    json_content = json.loads(content)\n",
    "\n",
    "    with open(cache_file, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(content)\n",
    "        print(f\"saved to cache file '{cache_file}' from cache ({len(content)} bytes)\")\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_way_coords(content, result):\n",
    "    #id = content[\"id\"]\n",
    "    type_ = content[\"type\"]\n",
    "\n",
    "\n",
    "    if type_ == \"way\":\n",
    "        for g in content[\"geometry\"]:\n",
    "            result.append([g[\"lat\"], g[\"lon\"]])\n",
    "    elif type_ == \"relation\":\n",
    "        for m in content[\"members\"]:\n",
    "            collect_way_coords(m, result)\n",
    "\n",
    "\n",
    "def json_way_to_pd(content: str,):\n",
    "    content_json = json.loads(content)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    element_count = len(content_json[\"elements\"])\n",
    "    if element_count == 0:\n",
    "        print(f\"element has no entries\")\n",
    "        return None\n",
    "\n",
    "    for e in content_json[\"elements\"]:\n",
    "        collect_way_coords(e, result)\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    df.columns = [\"lat\", \"lon\"]\n",
    "    return df\n",
    "\n",
    "def json_nodes_to_pd(content: str, skip_nodes_without_tags: bool = True):\n",
    "    content_json = json.loads(content)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    element_count = len(content_json[\"elements\"])\n",
    "    if element_count == 0:\n",
    "        print(f\"element has no entries\")\n",
    "        return None\n",
    "\n",
    "    for e in content_json[\"elements\"]:\n",
    "        id = e[\"id\"]\n",
    "        type_ = e[\"type\"]\n",
    "        if type_ == \"node\":\n",
    "            lat = e[\"lat\"]\n",
    "            lon = e[\"lon\"]\n",
    "        elif type_ == \"way\":\n",
    "            lat = e[\"center\"][\"lat\"]\n",
    "            lon = e[\"center\"][\"lon\"]\n",
    "        else:\n",
    "            print(f\"skipping node {id} {type_}\")\n",
    "            continue\n",
    "\n",
    "        tags = e.get(\"tags\", None)\n",
    "        if tags is None:\n",
    "            name = None\n",
    "            alt_name = None\n",
    "\n",
    "            # skip entries without tags, most likely they are 'node's of a 'way', and therefore are duplicates\n",
    "            if skip_nodes_without_tags:\n",
    "                continue\n",
    "        else:\n",
    "            name = tags.get(\"name\", None)\n",
    "            alt_name = tags.get(\"alt_name\", None)\n",
    "\n",
    "        result.append([id, lat, lon, name, alt_name])\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    df.columns = [\"id\", \"lat\", \"lon\", \"name\", \"alt_name\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_centers(content: str):\n",
    "    content_json = json.loads(content)\n",
    "\n",
    "    result = []\n",
    "    nodes = {}\n",
    "    element_count = len(content_json[\"elements\"])\n",
    "    if element_count == 0:\n",
    "        # print(\"# %s has no element entries\" % (name ))\n",
    "        return None\n",
    "\n",
    "    for e in content_json[\"elements\"]:\n",
    "        # print(e)\n",
    "        id = e[\"id\"]\n",
    "        type_ = e[\"type\"]\n",
    "        if type_ == \"node\":\n",
    "            lat = e[\"lat\"]\n",
    "            lon = e[\"lon\"]\n",
    "            nodes[id] = (lat, lon)\n",
    "        elif type_ == \"way\":\n",
    "            building = e[\"tags\"][\"building\"]\n",
    "            address = \"\"\n",
    "            node_coordinates = []\n",
    "            lat = 0\n",
    "            lon = 0\n",
    "            node_count = len(e[\"nodes\"])\n",
    "            for way_node_id in e[\"nodes\"]:\n",
    "                way_node = nodes[way_node_id]\n",
    "                lat += way_node[0] / node_count\n",
    "                lon += way_node[1] / node_count\n",
    "                node_coordinates.append([way_node[0], way_node[1]])\n",
    "\n",
    "            result.append([id, lat, lon, building, address])\n",
    "\n",
    "        else:\n",
    "            print(\"skipping node %s %s\" % (id, type_))\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    df.columns = [\"id\", \"lat\", \"lon\", \"building\", \"address\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\\\"Steiermark\\\"]->.b;\n",
    "(\n",
    "nwr(area.b)[highway=bus_stop];\n",
    "nwr(area.b)[public_transport=stop_position];\n",
    "nwr(area.b)[public_transport=platform];\n",
    ");\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"bus_stops_styria\"\n",
    "\n",
    "data = json_nodes_to_pd(download_poi(query, tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "(\n",
    "//node(area.b)[railway=\"^(halt|station|stop)$\"];\n",
    "nwr(area.b)[railway=\"halt\"];\n",
    "nwr(area.b)[railway=\"station\"];\n",
    "nwr(area.b)[railway=\"stop\"];\n",
    ");\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"railway_stations_styria\"\n",
    "\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\\\"Steiermark\\\"]->.b;\n",
    "(\n",
    "nwr(area.b)[shop=supermarket];\n",
    "//way(area.b)[shop=supermarket];\n",
    ");\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"supermarkets_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\\\"Steiermark\\\"]->.b;\n",
    "(\n",
    "nwr(area.b)[amenity=pharmacy];\n",
    "//way(area.b)[amenity=pharmacy];\n",
    ");\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"pharmacy_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\\\"Steiermark\\\"]->.b;\n",
    "(\n",
    "nwr(area.b)[shop=\\\"chemist\\\"]; \n",
    ");\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "# drogerie in german\n",
    "tag = \"chemist_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\\\"Steiermark\\\"]->.b;\n",
    "(\n",
    "nwr(area.b)[shop=bakery];\n",
    "//way(area.b)[shop=bakery];\n",
    ");\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tag = \"bakery_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restaurants\n",
    "\n",
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\\\"Steiermark\\\"]->.b;\n",
    "(\n",
    "nwr(area.b)[amenity=restaurant];\n",
    "//way(area.b)[amenity=restaurant];\n",
    ");\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"restaurant_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shops\n",
    "# without supermarkets and bakeries\n",
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "((\n",
    "nwr(area.b)[shop]; \n",
    " - nwr(area.b)[shop=\"supermarket\"];\n",
    ");\n",
    "- nwr(area.b)[shop=\"bakery\"];\n",
    "\n",
    ");\n",
    "\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"shops_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motorway exits\n",
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\\\"Steiermark\\\"]->.b;\n",
    "// Select motorway links\n",
    "way(area.b)[highway=\"motorway_link\"]->.motorway_links;\n",
    "\n",
    "// Select primary roads\n",
    "\n",
    "way(area.b)[highway~\"^(primary|secondary|tertiary|trunk|trunk_link|primary_link|secondary_link|tertiary_link|unclassified)$\"]->.primary_roads;\n",
    "\n",
    "// Find intersecting nodes\n",
    "node(w.motorway_links)->.motorway_nodes;\n",
    "node(w.primary_roads)->.primary_nodes;\n",
    "node.motorway_nodes.primary_nodes;\n",
    "\n",
    "// Output the intersecting nodes\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"motorway_exits\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag,use_cache=False),skip_nodes_without_tags=False)\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gas stations\n",
    "#nwr -> is type agnostic and selects nodes, ways and relations\n",
    "\n",
    "query =\"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "(\n",
    "//nwr -> is type agnostic and selects nodes, ways and relations\n",
    "nwr (area.b)[amenity=fuel];\n",
    "//node(area.b)[amenity=fuel];\n",
    "//way(area.b)[amenity=fuel];\n",
    ");\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"gasstations_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=True))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motorways (Autobahnen und Zuführer)\n",
    "# query =\"\"\"\n",
    "# [out:json];\n",
    "# area[name=\"Steiermark\"]->.b;\n",
    "# way(area.b)[highway~\"^(motorway|trunk|motorway_link|trunk_link)$\"]->.hw;\n",
    "# node(w.hw);\n",
    "# //(._;>;);\n",
    "# out body;\n",
    "# \"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "way(area.b)[highway~\"^(motorway|trunk|motorway_link|trunk_link)$\"][!\"tunnel\"]->.hw;\n",
    "node(w.hw);\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"motorway_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag, use_cache=False),skip_nodes_without_tags=False)\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motorways (Autobahnen und Zuführer)\n",
    "query =\"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "way(area.b)[highway~\"^(primary|secondary)$\"][!\"tunnel\"]->.hw;\n",
    "node(w.hw);\n",
    "//(._;>;);\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "highway=\"motorway\"\n",
    "tunnel=\"yes\"\n",
    "\n",
    "tag = \"street_primary_secondary_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag,use_cache=False),skip_nodes_without_tags=False)\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soccer field/stadium\n",
    "\n",
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "(\n",
    "nw(area.b)[sport=soccer];\n",
    ");\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"soccer_fields_styria\"\n",
    "data = json_nodes_to_pd(\n",
    "    download_poi(query, tag, use_cache=True), skip_nodes_without_tags=False\n",
    ")\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from shapely.geometry import Polygon\n",
    "from area import area as calculate_geo_area\n",
    "import csv\n",
    "\n",
    "def calculate_area(data):\n",
    "    # Extract nodes and ways from the data\n",
    "    nodes = {el['id']: el for el in data['elements'] if el['type'] == 'node'}\n",
    "    ways = [el for el in data['elements'] if el['type'] == 'way']\n",
    "\n",
    "    areas = []\n",
    "\n",
    "    for way in ways:\n",
    "        coords = [(nodes[node_id]['lat'], nodes[node_id]['lon']) for node_id in way['nodes']]\n",
    "        # Create GeoJSON-like structure\n",
    "        geojson_polygon = {\n",
    "            'type': 'Polygon',\n",
    "            'coordinates': [[(lon, lat) for lat, lon in coords]]\n",
    "        }\n",
    "        # Calculate area using the 'area' library\n",
    "        area_size = calculate_geo_area(geojson_polygon)\n",
    "        polygon = Polygon([(lon, lat) for lat, lon in coords])\n",
    "        areas.append({\n",
    "            'id': way['id'],\n",
    "            'center_lat': polygon.centroid.y,\n",
    "            'center_lon': polygon.centroid.x,\n",
    "            'area': area_size\n",
    "        })\n",
    "        print(f\"Area of way {way['id']}: {area_size} square meters\")\n",
    "\n",
    "    return areas\n",
    "\n",
    "def save_to_csv(areas, filename='areas.csv'):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['id', 'center_lat', 'center_lon', 'area'])\n",
    "        for area in areas:\n",
    "            writer.writerow([area['id'], area['center_lat'], area['center_lon'], area['area']])\n",
    "\n",
    "# Example usage\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "overpass_query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "(\n",
    "  way(area.b)[sport=soccer];\n",
    ");\n",
    "(._;>;);\n",
    "out body;\n",
    "\"\"\"\n",
    "response = requests.post(overpass_url, data=overpass_query)\n",
    "data = response.json()\n",
    "\n",
    "areas = calculate_area(data)\n",
    "save_to_csv(areas, f\"{data_path.osm}/soccer_fields_areas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# railway rails\n",
    "\n",
    "query = \"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "way(area.b)[railway=rail][!\"tunnel\"]->.rails;\n",
    "node(w.rails);\n",
    "//(._;>;);\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"railway_rails_styria\"\n",
    "data = json_nodes_to_pd(\n",
    "    download_poi(query, tag, use_cache=False), skip_nodes_without_tags=False\n",
    ")\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rivers\n",
    "\n",
    "query =\"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "way(area.b)[waterway=river]->.rivers;\n",
    "node(w.rivers);\n",
    "//(._;>;);\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"rivers_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag,use_cache=False),skip_nodes_without_tags=False)\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streams\n",
    "\n",
    "query =\"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "way(area.b)[waterway=stream]->.streams;\n",
    "node(w.streams);\n",
    "//(._;>;);\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"streams_styria\"\n",
    "data = json_nodes_to_pd(download_poi(query,tag,use_cache=False),skip_nodes_without_tags=False)\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# houses\n",
    "#13.6054501606841 16.1110951871948\n",
    "y1= 13.5\n",
    "y2 = 16.2\n",
    "#46.649784205005204 47.7718630862253\n",
    "x1 = 46.6\n",
    "x2 = 47.8\n",
    "\n",
    "#[bbox:south,west,north,east]\n",
    "#[bbox:50.6,7.0,50.8,7.3]\n",
    "#\"[bbox:%.1f,%.1f,%.1f,%.1f]\" \n",
    "\n",
    "query =\"\"\"\n",
    "[out:json];\n",
    "nw[building](%.6f,%.6f,%.6f,%.6f);\n",
    "(._;>;);\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "x_step = (x2-x1)/20\n",
    "y_step = (y2-y1)/20\n",
    "print(x_step,y_step)\n",
    "houses = []\n",
    "for x in tqdm.tqdm(np.arange(x1, x2, x_step)):\n",
    "    for y in np.arange(y1, y2, y_step):\n",
    "        tag = \"houses_styria_x=%.6f_y=%.6f,stepx=%.6f,stepy=%.6f\" % (x,y,x_step,y_step)\n",
    "        part_query = query % (x, y, x + x_step, y + y_step)\n",
    "        #print(part_query)\n",
    "        #data = json_nodes_to_pd(download_poi(part_query,tag))\n",
    "        data = download_poi(part_query,tag, use_cache=True)\n",
    "        houses.append(building_centers(data))\n",
    "        \n",
    "        #f data is not None:\n",
    "        #   print(part_query)\n",
    "#houses_all = np.concatenate(houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_pd = pd.concat(houses, ignore_index=True)\n",
    "tag = \"houses_styria_center\"\n",
    "houses_pd.to_csv(f\"{data_path.osm}/{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = houses_pd['building'].value_counts()\n",
    "x\n",
    "#len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_pd[~houses_pd['building'].isin(['shed', 'garage', 'garages', 'barn', 'cabin', 'hut', 'collapsed', 'cowshed', 'pavilion', 'shelter', 'greenhouse'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all schools from  statistik austria\n",
    "#https://data.statistik.gv.at/web/catalog.jsp\n",
    "# https://www.data.gv.at/suche/?typeFilter%5B%5D=dataset&searchterm=schulen&searchin=data\n",
    "\n",
    "# Schools in Austria\n",
    "# https://www.data.gv.at/katalog/dataset/6f3e9528-33d5-37e4-ba0a-a42e041ad41d#resources\n",
    "\n",
    "# Schools by type in Styria\n",
    "# https://www.data.gv.at/katalog/dataset/7b86e565-cde3-4e18-804c-d175748e07b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the kmz file, in case it isn't available anymore, a copy is in the data folder\n",
    "url_school_locations = \"https://service.stmk.gv.at/ogd/OGD_Data_ABT17/geoinformation/Bildungsstandorte.kmz\"\n",
    "response = requests.get(url_school_locations)\n",
    "kmz_path = f\"{data_path.statistik_austria}/Bildungsstandorte.kmz\"\n",
    "with open(kmz_path, 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "from fastkml import kml\n",
    "\n",
    "def extract_data_from_kml_record(record):\n",
    "    einrichtung = None\n",
    "    typ_lang = None\n",
    "    for element in record.extended_data.elements:\n",
    "        #print(element)\n",
    "        for data in element.data:\n",
    "            #print(\"dataline\", data)\n",
    "            if data.get(\"name\",\"\") == \"EINRICHTUNG\":\n",
    "                einrichtung = data.get(\"value\",\"\")\n",
    "            elif data.get(\"name\",\"\") == \"TYP_LANG\":\n",
    "                typ_lang = data.get(\"value\",\"\")\n",
    "    return einrichtung, typ_lang\n",
    "\n",
    "\n",
    "# read KML content\n",
    "kmz_file = f\"{data_path.statistik_austria}/Bildungsstandorte.kmz\"\n",
    "\n",
    "kmz = zipfile.ZipFile(kmz_file, 'r')  # extract zip file first, then read kmz file inside the extracted folder\n",
    "\n",
    "kml_content = kmz.open('doc.kml', 'r').read()  # kml content\n",
    "# FIX CONTENT\n",
    "kml_content_fixed = kml_content.replace(b'xsd:string', b'string')\n",
    "kml_content_fixed = kml_content_fixed.replace(b'xsd:int', b'int')\n",
    "\n",
    "print(\"len\", len(kml_content_fixed))\n",
    "# create KML object\n",
    "k = kml.KML()\n",
    "k.from_string(kml_content_fixed)\n",
    "\n",
    "# read features from docs to folders to records and then extract geometries - in my case, Shapely points\n",
    "docs = list(k.features())\n",
    "folders = []\n",
    "for d in docs:\n",
    "    folders.extend(list(d.features()))\n",
    "records = []\n",
    "for f in folders:\n",
    "    records.extend(list(f.features()))\n",
    "geoms = [element.geometry for element in records]  # extract geometry\n",
    "# Extract data from records\n",
    "data = []\n",
    "for record in records:\n",
    "    name = record.name\n",
    "    description = record.description\n",
    "    coordinates = record.geometry.coords[0]\n",
    "    einrichtung, typ_lang = extract_data_from_kml_record(record)\n",
    "\n",
    "    data.append([name, einrichtung, typ_lang, coordinates[1], coordinates[0]])  # lat, lon\n",
    "\n",
    "# Create a DataFrame\n",
    "df_records = pd.DataFrame(data, columns=['Name', 'Type', 'Type_Long', 'Latitude', 'Longitude'])\n",
    "print(df_records)\n",
    "# # plot the geoms - latitude (y), longitude (x)\n",
    "# import folium\n",
    "# import pandas as pd\n",
    "# mapit = folium.Map()\n",
    "# for geom in geoms:\n",
    "#     #print(geom)\n",
    "#     folium.Marker(location=(geom.y, geom.x), fill_color='#43d9de', radius=2).add_to(mapit)\n",
    "# mapit.save('map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records[(df_records['Type'] == 'Schule')].to_csv(f\"{data_path.statistik_austria}/schools_styria.csv\")\n",
    "df_records[(df_records['Type_Long'] == 'Volksschule')].to_csv(f\"{data_path.statistik_austria}/elementary_schools_styria.csv\")\n",
    "\n",
    "secondary_types_german = [\"Mittelschule\", \"Allgemeinbildende höhere Schule\",\"Lehranstalt für wirtschaftliche Berufe, Sozialberufe und Tourismus\", \"Polytechnische Schule\", \"Land- und forstwirtschaftliche mittlere Schule\", \"Kaufmännische mittlere oder höhere Schule\", \"Technisch-gewerbliche Schule\"]\n",
    "#secondary (or middleschool)\n",
    "df_records[(df_records['Type_Long'].isin(secondary_types_german))].to_csv(f\"{data_path.statistik_austria}/secondary_schools_styria.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# administrative border of styria for masking the region\n",
    "\n",
    "query =\"\"\"\n",
    "[out:json];\n",
    "area[name=\"Steiermark\"]->.b;\n",
    "(\n",
    "\twr(area.b)[boundary=\"administrative\"][admin_level=4];\n",
    ");\n",
    "out geom;\n",
    "\"\"\"\n",
    "\n",
    "tag = \"admin_border_styria\"\n",
    "data = json_way_to_pd(download_poi(query,tag))\n",
    "data.to_csv(f\"{data_path.osm}/{tag}.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
